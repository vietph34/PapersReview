## Energy-Based Model for NLP:
- [Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning
, EMNLP 2022](https://arxiv.org/abs/2203.06875v2)
## ü•≥ Sentence Embedding:
- [Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning](https://arxiv.org/abs/2202.13196)

- [PairwiseSupervisedContrastiveLearningofSentenceRepresentations](https://aclanthology.org/2021.emnlp-main.467.pdf)
## üòÉ Phrase Embedding:
- [PhraseBERT](https://aclanthology.org/2021.emnlp-main.846/)

- [DensePhrase](https://arxiv.org/abs/2012.12624)
## üîç Question Answering:
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Neurips 2020](https://arxiv.org/abs/2005.11401) 
- [Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering, arxiv, 2021](https://arxiv.org/abs/2007.01282)
- [End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering, Neurips2021](https://arxiv.org/abs/2106.05346)
- [Learning to Retrieve Passages without Supervision, NAACL 2022](https://arxiv.org/abs/2112.07708)
- [Long Context Question Answering via Supervised Contrastive Learning, NAACL 2022](https://aclanthology.org/2022.naacl-main.207.pdf)
- [Improving Passage Retrieval with Zero-Shot Question Generation, EMNLP 2022](https://arxiv.org/abs/2204.07496?context=cs.IR)
- [Questions Are All You Need to Train a Dense Passage Retriever](https://arxiv.org/abs/2206.10658)
## üêù Self-supervised learning:
- [PiCO: Contrastive Label Disambiguation for Partial Label Learning, ICLR 2022](https://arxiv.org/abs/2201.08984)

## üê£ Prompting:
- [Automatic Label Sequence Generationfor Prompting Sequence-to-sequence Models](https://arxiv.org/pdf/2209.09401.pdf)
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
- [SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer](https://aclanthology.org/2022.acl-long.346/)
- [Using natural language prompts for machine translation](https://arxiv.org/pdf/2202.11822.pdf)
- [Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation](https://arxiv.org/pdf/2302.07856.pdf)
- [Prompting Large Language Model for Machine Translation: A Case Study](https://arxiv.org/pdf/2301.07069.pdf)
- [Zero-shot Domain Adaptation for Neural Machine Translation with Retrieved Phrase-level Prompts](https://arxiv.org/abs/2209.11409)

## ü¶ö Domain Adaptation:
- [Soft Alignment Objectives for Robust Adaptation of Language Generation](https://arxiv.org/pdf/2211.16550.pdf)
